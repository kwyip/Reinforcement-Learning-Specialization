<meta charset="utf-8"/>
<h3>
 Question 1
</h3>
<co-content>
 <p>
  Which approach ensures continual (never-ending) exploration? (
  <strong>
   Select all that apply
  </strong>
  )
 </p>
 <p>
 </p>
</co-content>
<form>
 <label>
  <input name="0" type="checkbox"/>
  <co-content>
   <span>
    Exploring starts
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="0" type="checkbox"/>
  <co-content>
   <span>
    On-policy learning with a
    <strong>
     deterministic
    </strong>
    policy
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="0" type="checkbox"/>
  <co-content>
   <span hasmath="true">
    On-policy learning with an $$\epsilon$$-soft policy
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="0" type="checkbox"/>
  <co-content>
   <span hasmath="true">
    Off-Policy learning with an $$\epsilon$$-soft behavior policy and a
    <strong>
     deterministic
    </strong>
    target policy
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="0" type="checkbox"/>
  <co-content>
   <span hasmath="true">
    Off-Policy learning with an $$\epsilon$$-soft target policy and a
    <strong>
     deterministic
    </strong>
    behavior policy
   </span>
  </co-content>
  <br/>
 </label>
</form>
<hr/>
<h3>
 Question 2
</h3>
<co-content>
 <p>
  When can Monte Carlo methods, as defined in the course, be applied? (Select all that apply)
 </p>
</co-content>
<form>
 <label>
  <input name="1" type="checkbox"/>
  <co-content>
   <span>
    When the problem is
    <strong>
     continuing
    </strong>
    and given a batch of data containing sequences of states, actions, and rewards
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="1" type="checkbox"/>
  <co-content>
   <span>
    When the problem is
    <strong>
     continuing
    </strong>
    and there is a model that produces samples of the next state and reward
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="1" type="checkbox"/>
  <co-content>
   <span>
    When the problem is
    <strong>
     episodic
    </strong>
    and given a batch of data containing sample episodes (sequences of states, actions, and rewards)
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="1" type="checkbox"/>
  <co-content>
   <span>
    When the problem is
    <strong>
     episodic
    </strong>
    and there is a model that produces samples of the next state and reward
   </span>
  </co-content>
  <br/>
 </label>
</form>
<hr/>
<h3>
 Question 3
</h3>
<co-content>
 <p>
  Which of the following learning settings are examples of off-policy learning? (Select all that apply)
 </p>
</co-content>
<form>
 <label>
  <input name="2" type="checkbox"/>
  <co-content>
   <span>
    Learning the optimal policy while continuing to explore
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="2" type="checkbox"/>
  <co-content>
   <span>
    Learning from data generated by a human expert
   </span>
  </co-content>
  <br/>
 </label>
</form>
<hr/>
<h3>
 Question 4
</h3>
<co-content>
 <p hasmath="true">
  Which of the following is a requirement
  <em>
   on the behaviour policy
  </em>
  $$b$$ for using
  <strong>
   off-policy
  </strong>
  Monte Carlo policy evaluation?  This is called the
  <em>
   assumption of coverage
  </em>
  .
 </p>
</co-content>
<form>
 <label>
  <input name="3" type="radio"/>
  <co-content>
   <span hasmath="true">
    For each state $$s$$ and action $$a$$, if $$b(a\mid s)&gt;0$$ then $$\pi(a\mid s)&gt;0$$
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="3" type="radio"/>
  <co-content>
   <span hasmath="true">
    All actions have non-zero probabilities under $$\pi$$
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="3" type="radio"/>
  <co-content>
   <span hasmath="true">
    For each state $$s$$ and action $$a$$, if $$\pi(a\mid s)&gt;0$$ then $$b(a\mid s)&gt;0$$
   </span>
  </co-content>
  <br/>
 </label>
</form>
<hr/>
<h3>
 Question 5
</h3>
<co-content>
 <p hasmath="true">
  When is it possible to determine a policy that is greedy with respect to the value functions $$v_{\pi}, q_{\pi}$$ for the policy $$\pi$$? (Select all that apply)
 </p>
</co-content>
<form>
 <label>
  <input name="4" type="checkbox"/>
  <co-content>
   <span hasmath="true">
    When state values $$v_{\pi}$$ and a model are available
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="4" type="checkbox"/>
  <co-content>
   <span hasmath="true">
    When state values $$v_{\pi}$$ are available but no model is available.
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="4" type="checkbox"/>
  <co-content>
   <span hasmath="true">
    When action values $$q_{\pi}$$ and a model are available
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="4" type="checkbox"/>
  <co-content>
   <span hasmath="true">
    When action values $$q_{\pi}$$ are available but no model is available.
   </span>
  </co-content>
  <br/>
 </label>
</form>
<hr/>
<h3>
 Question 6
</h3>
<co-content>
 <p>
  Monte Carlo methods in Reinforcement Learning work by...
 </p>
 <p>
  Hint: recall we used the term
  <em>
   sweep
  </em>
  in dynamic programming to discuss updating all the states systematically. This is
  <strong>
   not
  </strong>
  the same as visiting a state.
 </p>
</co-content>
<form>
 <label>
  <input name="5" type="radio"/>
  <co-content>
   <span>
    Performing
    <strong>
     sweeps
    </strong>
    through the state set
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="5" type="radio"/>
  <co-content>
   <span>
    Averaging sample returns
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="5" type="radio"/>
  <co-content>
   <span>
    <strong>
     Planning
    </strong>
    with a model of the environment
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="5" type="radio"/>
  <co-content>
   <span>
    Averaging sample rewards
   </span>
  </co-content>
  <br/>
 </label>
</form>
<hr/>
<h3>
 Question 7
</h3>
<co-content>
 <p hasmath="true">
  Suppose the state $$s$$ has been visited three times, with corresponding returns $$8$$, $$4$$, and $$3$$. What is the current Monte Carlo estimate for the value of $$s$$?
 </p>
</co-content>
<form>
 <label>
  <input name="6" type="radio"/>
  <co-content>
   <span hasmath="true">
    $$3$$
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="6" type="radio"/>
  <co-content>
   <span hasmath="true">
    $$15$$
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="6" type="radio"/>
  <co-content>
   <span hasmath="true">
    $$5$$
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="6" type="radio"/>
  <co-content>
   <span hasmath="true">
    $$3.5$$
   </span>
  </co-content>
  <br/>
 </label>
</form>
<hr/>
<h3>
 Question 8
</h3>
<co-content>
 <p>
  When does Monte Carlo prediction perform its first update?
 </p>
</co-content>
<form>
 <label>
  <input name="7" type="radio"/>
  <co-content>
   <span>
    After the first time step
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="7" type="radio"/>
  <co-content>
   <span>
    After every state is visited at least once
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="7" type="radio"/>
  <co-content>
   <span>
    At the end of the first episode
   </span>
  </co-content>
  <br/>
 </label>
</form>
<hr/>
<h3>
 Question 9
</h3>
<co-content>
 <p>
  In Monte Carlo prediction of state-values,
  <strong>
   memory
  </strong>
  requirements depend on (Select all that apply).
 </p>
 <p>
  Hint: think of the two data structures used in the algorithm
 </p>
</co-content>
<form>
 <label>
  <input name="8" type="checkbox"/>
  <co-content>
   <span>
    The number of states
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="8" type="checkbox"/>
  <co-content>
   <span>
    The number of possible actions in each state
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="8" type="checkbox"/>
  <co-content>
   <span>
    The length of episodes
   </span>
  </co-content>
  <br/>
 </label>
</form>
<hr/>
<h3>
 Question 10
</h3>
<co-content>
 <p hasmath="true">
  In an $$\epsilon$$-greedy policy over $$\mathcal{A}$$ actions, what is the probability of the highest valued action if there are no other actions with the same value?
 </p>
</co-content>
<form>
 <label>
  <input name="9" type="radio"/>
  <co-content>
   <span hasmath="true">
    $$1-\epsilon$$
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="9" type="radio"/>
  <co-content>
   <span hasmath="true">
    $$\epsilon$$
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="9" type="radio"/>
  <co-content>
   <span hasmath="true">
    $$1-\epsilon \frac{\epsilon}{\mathcal{A}}$$
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="9" type="radio"/>
  <co-content>
   <span hasmath="true">
    $$\frac{\epsilon}{\mathcal{A}}$$
   </span>
  </co-content>
  <br/>
 </label>
</form>
<hr/>
<style>
 body {
    padding: 50px 85px 50px 85px;
}

table th, table td {
    border: 1px solid #e0e0e0;
    padding: 5px 20px;
    text-align: left;
}
input {
    margin: 10px;
}
}
th {
    font-weight: bold;
}
td, th {
    display: table-cell;
    vertical-align: inherit;
}
img {
    height: auto;
    max-width: 100%;
}
pre {
    display: block;
    margin: 20px;
    background: #424242;
    color: #fff;
    font-size: 13px;
    white-space: pre-wrap;
    padding: 9.5px;
    margin: 0 0 10px;
    border: 1px solid #ccc;
}
</style>
<script async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$$','$$'], ['$','$'] ],
      displayMath: [ ["\\[","\\]"] ],
      processEscapes: true
    }
  });
</script>
